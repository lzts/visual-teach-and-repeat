{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependicies**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "\n",
    "from IPython.display import display,HTML,clear_output\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from math import pi as PI\n",
    "\n",
    "mpl.use('nbagg')\n",
    "\n",
    "from matplotlib import animation\n",
    "mpl.rc('animation', html='html5') #display animated plots inline\n",
    "\n",
    "from robmob.robot import Robot\n",
    "from robmob.sensors import KinectRGBSensor\n",
    "from robmob.sensors import KinectDepthSensor\n",
    "from robmob.visualization import Visualizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to Robot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_robot = '192.168.0.109'\n",
    "robot = Robot(ip_robot)\n",
    "robot.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kinect = KinectRGBSensor()\n",
    "robot.add_sensor(kinect)\n",
    "depth_sensor = KinectDepthSensor()\n",
    "robot.add_sensor(depth_sensor)\n",
    "\n",
    "qrTag_1 = cv2.imread('qr2.png', 0)          # trainImage1\n",
    "qrTag_2 = cv2.imread('qr.jpg', 0)          # trainImage2\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# | input parameter  |-> trainImage\n",
    "#                    |-> queryImage\n",
    "# -------------------------------------------------\n",
    "# | output parameter |-> (numberOfDMatch, listOfDMatchPoints) \n",
    "# -------------------------------------------------\n",
    "def get_inlines(queryImg, tag):\n",
    "    queryImage = np.array(queryImg)\n",
    "    MIN_MATCH_COUNT = 1\n",
    "\n",
    "    kp1, des1 = sift.detectAndCompute(queryImage,None)    \n",
    "    kp2, des2 = sift.detectAndCompute(tag,None)     \n",
    "    \n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.5*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) > MIN_MATCH_COUNT:\n",
    "        return len(good), good\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# | input parameter  |-> tag\n",
    "#                    |-> angle\n",
    "#                    |-> speed\n",
    "# -------------------------------------------------\n",
    "# | output parameter |-> \"the orientation of the robot must be toward the tag\"\n",
    "# -------------------------------------------------\n",
    "def scan_for_max_inlines(tag, angle, speed):\n",
    "    max_inlines = False\n",
    "    inlines = 0\n",
    "    val=[]\n",
    "    while not max_inlines:\n",
    "        robot.angular_movement(angle, speed)\n",
    "        image = kinect.peek_data()\n",
    "        time.sleep(1)\n",
    "        new_inlines = get_inlines(image, tag)[0]\n",
    "#         print(\"new_inlines \" , new_inlines, \"inlines: \", inlines)\n",
    "        kinect.buffer.clear()\n",
    "        \n",
    "        if inlines <= new_inlines:\n",
    "            inlines = new_inlines\n",
    "#             print(\"not max_inlines\")\n",
    "        else:\n",
    "            max_inlines = False\n",
    "#             print(\"yes\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# | input parameter  |-> list of DMatches (from SIFT algorithm)\n",
    "#                    |-> image where to find coords (queryImage)\n",
    "#                    |-> QR tag                     (trainImage)\n",
    "# -------------------------------------------------\n",
    "# | output parameter |-> list of coordonates\n",
    "# -------------------------------------------------\n",
    "def find_coords(dMatchList, queryImg, tag):\n",
    "    queryImage = np.array(queryImg)\n",
    "    \n",
    "    # Initiate SIFT detector\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(queryImage, None)\n",
    "    kp2, des2 = sift.detectAndCompute(tag, None)\n",
    "    \n",
    "    points = []\n",
    "    for dmatch in dMatchList:\n",
    "        coords = kp1[dmatch.queryIdx].pt\n",
    "        points.append(coords)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# | input parameter  |-> Depth data\n",
    "#                    |-> list of points (coordinates)\n",
    "# -------------------------------------------------\n",
    "# | output parameter |-> distance (z) to tag\n",
    "# -------------------------------------------------\n",
    "def get_z_from_listOfPoints(depth_data, points):\n",
    "    zInEachPoint = []\n",
    "    for point in points:\n",
    "        z = depth_data[(point[0], point[1])]\n",
    "        if z > 0 and z < 3 :\n",
    "            zInEachPoint.append(z)\n",
    "     \n",
    "    print('zInEachPoint', zInEachPoint) \n",
    "    avg = sum(zInEachPoint)/len(zInEachPoint)\n",
    "    \n",
    "    zInEachPointWithoutAberrant = []\n",
    "    for z in zInEachPoint:\n",
    "        if ((avg*0.8) < z < (avg*1.2)):\n",
    "            zInEachPointWithoutAberrant.append(z)\n",
    "    \n",
    "    \n",
    "    print(zInEachPointWithoutAberrant)\n",
    "    return sum(zInEachPointWithoutAberrant)/len(zInEachPointWithoutAberrant)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ***  MAIN  ***\n",
    "\n",
    "for tag in [qrTag_2, qrTag_2]:\n",
    "    time.sleep(1)\n",
    "    print('Let''s Do IT')\n",
    "    time.sleep(1)\n",
    "    scan_for_max_inlines(tag, PI/8,1)\n",
    "    scan_for_max_inlines(tag, -PI/16,0.5)\n",
    "    time.sleep(1)\n",
    "    queryImage = kinect.peek_data()\n",
    "    time.sleep(1)\n",
    "    dMatchList = get_inlines(queryImage, tag)[1]\n",
    "    points = find_coords(dMatchList, queryImage, tag)\n",
    "    time.sleep(1)\n",
    "    depth_data = depth_sensor.peek_data()\n",
    "    d = get_z_from_listOfPoints(depth_data, points)\n",
    "    time.sleep(1)\n",
    "    robot.linear_movement_precise(d*0.8, 0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
